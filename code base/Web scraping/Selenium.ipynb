{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paging with Selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver \n",
    "from selenium.webdriver.common.by import By \n",
    "from time import sleep\n",
    "from random import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the programmable browser\n",
    "browser = webdriver.Chrome(executable_path=\"C:\\\\Users\\\\llewe\\\\Desktop\\\\tidy\\\\chromedriver\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the books, page by page\n",
    "currentPage = 1\n",
    "maxPages = 5\n",
    "done = False\n",
    "url = \"https://books.toscrape.com/\"\n",
    "\n",
    "while currentPage <= maxPages:\n",
    "    # Add a random wait time so we don't overload the website and we don't look like a bot\n",
    "    sleep(random()*3)\n",
    "    \n",
    "    # Load the page\n",
    "    print(\"Loading\", url)\n",
    "    browser.get(url)\n",
    "\n",
    "    # Find the location of the list of books\n",
    "    books = browser.find_element(By.TAG_NAME, \"ol\").find_elements(By.TAG_NAME, \"li\")\n",
    "    \n",
    "    # Extract all the books\n",
    "    for book in books:\n",
    "        title = book.find_element(By.TAG_NAME, \"h3\").find_element(By.TAG_NAME, \"a\").get_attribute(\"title\")\n",
    "        print(\"\\t\", title)\n",
    "    \n",
    "    # \"Click\" the next button\n",
    "    nextButton = browser.find_element(By.CLASS_NAME, \"next\")\n",
    "    currentPage += 1\n",
    "    url = nextButton.find_element(By.TAG_NAME, \"a\").get_attribute(\"href\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrolling with Selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver \n",
    "from selenium.webdriver.common.by import By \n",
    "from time import sleep\n",
    "from random import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the programmable browser\n",
    "browser = webdriver.Chrome(executable_path=\"C:\\\\Users\\\\llewe\\\\Desktop\\\\tidy\\\\chromedriver\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the articles by scrolling\n",
    "currentPage = 1\n",
    "maxPages = 10\n",
    "done = False\n",
    "url = \"https://www.reddit.com/r/news/\"\n",
    "\n",
    "# Load the page\n",
    "print(\"Loading\", url)\n",
    "browser.get(url)\n",
    "    \n",
    "while currentPage <= maxPages:\n",
    "    print(\"Page\", currentPage)\n",
    "    \n",
    "    # Add a random wait time so we don't overload the website and we don't look like a bot\n",
    "    sleep(random()*3)\n",
    "    \n",
    "    # Scroll down the height of the page\n",
    "    browser.execute_script(\"window.scrollTo(0,document.body.scrollHeight)\")\n",
    "\n",
    "    currentPage += 1\n",
    "\n",
    "    \n",
    "# Now we have all the pages, extrac the titles\n",
    "articles = browser.find_elements(By.TAG_NAME, \"h3\")\n",
    "for article in articles:\n",
    "    print(article.get_attribute(\"innerHTML\"))\n",
    "\n",
    "  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
